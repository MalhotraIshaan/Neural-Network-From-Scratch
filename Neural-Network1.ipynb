{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a Single Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "# Inputs to the neuron\n",
    "inputs = [1, 2, 3]\n",
    "\n",
    "# Weights associated with each input\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "\n",
    "# Bias term added to the weighted sum\n",
    "bias = 2\n",
    "\n",
    "# Calculate the output of the neuron using the formula:\n",
    "# output = (input1 * weight1) + (input2 * weight2) + (input3 * weight3) + bias\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 3.465]\n"
     ]
    }
   ],
   "source": [
    "# Inputs to the neurons\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "# Weights for the first neuron\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "\n",
    "# Weights for the second neuron\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "\n",
    "# Weights for the third neuron\n",
    "weights3 = [-0.26, 0.27, 0.17, 0.87]\n",
    "\n",
    "# Bias for the first neuron\n",
    "bias1 = 2\n",
    "\n",
    "# Bias for the second neuron\n",
    "bias2 = 3\n",
    "\n",
    "# Bias for the third neuron\n",
    "bias3 = 0.5\n",
    "\n",
    "# Calculate the output of each neuron using the formula:\n",
    "# output = (input1 * weight1) + (input2 * weight2) + (input3 * weight3) + (input4 * weight4) + bias\n",
    "output = [\n",
    "    #neuron1\n",
    "    inputs[0] * weights1[0] + inputs[1] * weights1[1] + inputs[2] * weights1[2] + inputs[3] * weights1[3] + bias1,\n",
    "    #neuron2\n",
    "    inputs[0] * weights2[0] + inputs[1] * weights2[1] + inputs[2] * weights2[2] + inputs[3] * weights2[3] + bias2,\n",
    "    #neuron3\n",
    "    inputs[0] * weights3[0] + inputs[1] * weights3[1] + inputs[2] * weights3[2] + inputs[3] * weights3[3] + bias3\n",
    "]\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 3.465]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1], [0.5, -0.91, 0.26, -0.5], [-0.26, 0.27, 0.17, 0.87]]\n",
    "biases=[2,3,0.5]\n",
    "\n",
    "# Initialize a list to store the outputs of each neuron\n",
    "layer_outputs = []\n",
    "\n",
    "# Calculate the output for each neuron\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    # Calculate the weighted sum for the current neuron\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    # Add the bias term\n",
    "    neuron_output += neuron_bias\n",
    "    # Append the result to the list of layer outputs\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "# Print the outputs of the layer\n",
    "print(layer_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single neuron with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [1.0, 2.0, 3.0, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "# Calculate the output using NumPy's dot product function\n",
    "# np.dot(inputs, weights) calculates the weighted sum of inputs\n",
    "# Adding the bias to the weighted sum\n",
    "output = np.dot(weights,inputs) + bias\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A layer of neurons with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3  7.8  2.19]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [1, 3, 4, 5]\n",
    "weights = [\n",
    "    [0.1, -0.3, 0.4, 0.5],  # Weights for the first neuron\n",
    "    [0.3, 0.2, -0.9, 1.5],  # Weights for the second neuron\n",
    "    [0.19, 0.23, 0.14, -0.05]  # Weights for the third neuron\n",
    "]\n",
    "\n",
    "biases = [2, 3, 1]\n",
    "\n",
    "# Calculate the output for each neuron using NumPy\n",
    "# np.dot(weights, inputs) calculates the weighted sum for each neuron\n",
    "# biases is added to the corresponding weighted sum for each neuron\n",
    "output = np.dot(weights, inputs) + biases\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A layer of neurons & batch of data with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.3    7.8    2.19 ]\n",
      " [ 1.74   8.51   2.264]\n",
      " [ 1.91  -1.43   1.879]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [\n",
    "    [1, 3, 4, 5],          # Example 1\n",
    "    [3.0, 4.0, -0.9, 2.0], # Example 2\n",
    "    [-1.7, 2.9, 3.5, -0.9] # Example 3\n",
    "]\n",
    "weights = [\n",
    "    [0.1, -0.3, 0.4, 0.5],  # Weights for the first neuron\n",
    "    [0.3, 0.2, -0.9, 1.5],  # Weights for the second neuron\n",
    "    [0.19, 0.23, 0.14, -0.05]  # Weights for the third neuron\n",
    "]\n",
    "biases = [2, 3, 1]\n",
    "\n",
    "# Calculate the output for each example in the batch\n",
    "# np.array(weights).T transposes the weights matrix to align with the inputs\n",
    "# np.dot(inputs, np.array(weights).T) computes the dot product between inputs and transposed weights\n",
    "# biases is added to the corresponding weighted sum for each neuron\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.314   -2.276    3.2374 ]\n",
      " [-1.4234  -0.0986   5.52534]\n",
      " [-0.5324  -1.0406  -1.76196]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = [\n",
    "    [1, 3, 4, 5],          # Example 1\n",
    "    [3.0, 4.0, -0.9, 2.0], # Example 2\n",
    "    [-1.7, 2.9, 3.5, -0.9] # Example 3\n",
    "]\n",
    "\n",
    "# Weights for the first layer of neurons\n",
    "weights1 = [\n",
    "    [0.1, -0.3, 0.4, 0.5],  # Weights for the first neuron in layer 1\n",
    "    [0.3, 0.2, -0.9, 1.5],  # Weights for the second neuron in layer 1\n",
    "    [0.19, 0.23, 0.14, -0.05]  # Weights for the third neuron in layer 1\n",
    "]\n",
    "\n",
    "# Biases for the first layer of neurons\n",
    "biases1 = [2, 3, 1]\n",
    "\n",
    "# Weights for the second layer of neurons\n",
    "weights2 = [\n",
    "    [0.3, -0.1, 0.4],  # Weights for the first neuron in layer 2\n",
    "    [-0.6, 0.1, -0.4], # Weights for the second neuron in layer 2\n",
    "    [-0.5, 0.73, -0.14]  # Weights for the third neuron in layer 2\n",
    "]\n",
    "\n",
    "# Biases for the second layer of neurons\n",
    "biases2 = [-2, 1, 0.5]\n",
    "\n",
    "# Calculate the output of the first layer\n",
    "# np.dot(inputs, np.array(weights1).T) calculates the weighted sum for each neuron in layer 1\n",
    "# biases1 is added to the corresponding weighted sum for each neuron in layer 1\n",
    "layer1_outputs = np.dot(inputs, np.array(weights1).T) + biases1\n",
    "\n",
    "# Calculate the output of the second layer\n",
    "# np.dot(layer1_outputs, np.array(weights2).T) calculates the weighted sum for each neuron in layer 2\n",
    "# biases2 is added to the corresponding weighted sum for each neuron in layer 2\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "\n",
    "# Print the output of the second layer\n",
    "print(layer2_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of layer 1\n",
      "[[ 0.9223807  -0.3837693  -1.03204147  0.8075479   1.2598186  -0.11459155]\n",
      " [ 0.90337122 -0.12226562 -0.2982118   0.93720039  0.65630495  0.15855728]\n",
      " [ 0.21382385  0.00753456  0.18880229 -0.20391794  0.1694162   0.58286602]]\n",
      "Output of layer 2\n",
      "[[0.42159818]\n",
      " [0.32781235]\n",
      " [0.16372961]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "X = [\n",
    "    [1, 3, 4, 5],          # Example 1\n",
    "    [3.0, 4.0, -0.9, 2.0], # Example 2\n",
    "    [-1.7, 2.9, 3.5, -0.9] # Example 3\n",
    "]\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the Layer_Dense class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the output of the layer\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Create instances of Layer_Dense\n",
    "layer1 = Layer_Dense(4, 6)  # Layer with 4 inputs and 6 neurons\n",
    "layer2 = Layer_Dense(6, 1)  # Layer with 6 inputs and 1 neuron\n",
    "\n",
    "# Perform forward pass through layer 1\n",
    "layer1.forward(X)\n",
    "print(\"Output of layer 1\")\n",
    "print(layer1.output)\n",
    "\n",
    "# Perform forward pass through layer 2\n",
    "layer2.forward(layer1.output)\n",
    "print(\"Output of layer 2\")\n",
    "print(layer2.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layer Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 6.3385908e-04 0.0000000e+00 7.0642207e-05\n",
      "  9.0164586e-04]\n",
      " [0.0000000e+00 3.9957227e-05 9.2750194e-04 0.0000000e+00 6.5028836e-04\n",
      "  1.0838244e-03]\n",
      " ...\n",
      " [9.2587270e-02 0.0000000e+00 2.7677365e-02 6.0882296e-02 0.0000000e+00\n",
      "  6.9344796e-02]\n",
      " [1.4715683e-01 0.0000000e+00 1.5908248e-03 6.8911701e-02 0.0000000e+00\n",
      "  2.9615715e-02]\n",
      " [7.8694962e-02 0.0000000e+00 3.4270126e-02 5.8806412e-02 0.0000000e+00\n",
      "  7.9367116e-02]]\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the nnfs library\n",
    "nnfs.init()\n",
    "\n",
    "# Generate dataset\n",
    "X, y = spiral_data(100, 3)\n",
    "\n",
    "# Define the Layer_Dense class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the output of the layer\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Define the Activation_Relu class\n",
    "class Activation_Relu:\n",
    "    def forward(self, inputs):\n",
    "        # Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Create instances of Layer_Dense and Activation_Relu\n",
    "layer1 = Layer_Dense(2, 6)  # Layer with 2 inputs and 6 neurons\n",
    "activation1 = Activation_Relu()\n",
    "\n",
    "# Perform forward pass through the layer and activation\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "# Print the output of the ReLU activation\n",
    "print(activation1.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "Normalized values (softmax probabilities):\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "Sum of normalized values:\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the nnfs library\n",
    "nnfs.init()\n",
    "\n",
    "# Example layer outputs\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# Compute the exponential values of the layer outputs\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print(\"Exponentiated values:\")\n",
    "print(exp_values)\n",
    "\n",
    "# Normalize the exponential values to get probabilities (softmax)\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(\"Normalized values (softmax probabilities):\")\n",
    "print(norm_values)\n",
    "\n",
    "# Check that the sum of the normalized values is 1\n",
    "print(\"Sum of normalized values:\")\n",
    "print(np.sum(norm_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized values (softmax probabilities):\n",
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the nnfs library\n",
    "nnfs.init()\n",
    "\n",
    "# Example layer outputs for multiple samples\n",
    "layer_outputs = [\n",
    "    [4.8, 1.21, 2.385],  # Sample 1\n",
    "    [8.9, -1.81, 0.2],   # Sample 2\n",
    "    [1.41, 1.051, 0.026] # Sample 3\n",
    "]\n",
    "\n",
    "# Compute the exponential values for each element in the layer outputs\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "# Normalize the exponential values to get probabilities (softmax) for each sample\n",
    "# axis=1 ensures normalization across each row (i.e., each sample)\n",
    "# keepdims=True ensures that the output has the same number of dimensions as the input\n",
    "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "# Print the normalized values\n",
    "print(\"Normalized values (softmax probabilities):\")\n",
    "print(norm_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]\n",
      " [0.33328417 0.33328718 0.33342862]\n",
      " [0.33318216 0.33319145 0.33362636]\n",
      " [0.33318278 0.33319202 0.33362517]\n",
      " [0.33314922 0.33316055 0.3336902 ]\n",
      " [0.3331059  0.33311984 0.3337743 ]\n",
      " [0.3330813  0.3330968  0.33382186]\n",
      " [0.33311027 0.33312503 0.33376473]\n",
      " [0.3330537  0.33307084 0.33387548]\n",
      " [0.33300948 0.33302936 0.3339612 ]\n",
      " [0.33301342 0.33303303 0.33395353]\n",
      " [0.33299845 0.333019   0.3339825 ]\n",
      " [0.33312678 0.33318865 0.3336846 ]\n",
      " [0.3329409  0.33296496 0.33409408]\n",
      " [0.33299428 0.33303145 0.33397427]\n",
      " [0.3328737  0.33290187 0.33422446]\n",
      " [0.3328757  0.33290377 0.33422053]\n",
      " [0.33309552 0.33318356 0.33372095]\n",
      " [0.33317605 0.33328283 0.3335411 ]\n",
      " [0.332894   0.33293876 0.33416727]\n",
      " [0.3325638  0.33270204 0.3347342 ]\n",
      " [0.33281362 0.33284548 0.3343409 ]\n",
      " [0.33314455 0.33327034 0.33358508]\n",
      " [0.3331285  0.333258   0.33361349]\n",
      " [0.33241627 0.33257595 0.33500782]\n",
      " [0.33234823 0.33250993 0.3351418 ]\n",
      " [0.3327268  0.33290118 0.334372  ]\n",
      " [0.33251256 0.3326966  0.33479083]\n",
      " [0.33303848 0.33320293 0.33375862]\n",
      " [0.33289513 0.33302    0.33408487]\n",
      " [0.3324894  0.33269098 0.3348196 ]\n",
      " [0.33217454 0.33237296 0.33545244]\n",
      " [0.33198893 0.3321154  0.3358957 ]\n",
      " [0.33197358 0.33208773 0.33593866]\n",
      " [0.33205435 0.3322675  0.33567816]\n",
      " [0.33193922 0.33214495 0.33591586]\n",
      " [0.33208537 0.33231774 0.3355969 ]\n",
      " [0.33213225 0.33237374 0.33549398]\n",
      " [0.33214602 0.33239442 0.33545953]\n",
      " [0.33266547 0.3326531  0.33468142]\n",
      " [0.33167848 0.33184558 0.33647597]\n",
      " [0.3316745  0.33181593 0.33650956]\n",
      " [0.33160535 0.3318128  0.33658186]\n",
      " [0.33236307 0.33234516 0.33529174]\n",
      " [0.33152688 0.3317393  0.33673382]\n",
      " [0.33195725 0.3319697  0.33607304]\n",
      " [0.33166933 0.33175212 0.33657855]\n",
      " [0.33258075 0.3325668  0.33485246]\n",
      " [0.3320807  0.33205754 0.3358618 ]\n",
      " [0.33174273 0.3317812  0.33647612]\n",
      " [0.3326942  0.33268243 0.33462337]\n",
      " [0.33327198 0.33327082 0.33345714]\n",
      " [0.33312216 0.33311826 0.33375955]\n",
      " [0.33332214 0.33332282 0.33335507]\n",
      " [0.3328328  0.33282354 0.33434367]\n",
      " [0.33328426 0.33328333 0.3334324 ]\n",
      " [0.33317587 0.33317295 0.33365116]\n",
      " [0.33320865 0.33321625 0.33357516]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33270696 0.33269536 0.33459768]\n",
      " [0.3323478  0.33240813 0.3352441 ]\n",
      " [0.33259606 0.33264124 0.33476272]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3318191  0.33191153 0.33626938]\n",
      " [0.3324854  0.33253726 0.33497733]\n",
      " [0.33177653 0.33187154 0.33635193]\n",
      " [0.33155695 0.3316653  0.33677778]\n",
      " [0.33158025 0.33168715 0.3367326 ]\n",
      " [0.33162922 0.3317332  0.33663756]\n",
      " [0.33193487 0.3320203  0.3360448 ]\n",
      " [0.33149302 0.33160523 0.33690175]\n",
      " [0.33148253 0.33159536 0.33692214]\n",
      " [0.3314542  0.33156872 0.3369771 ]\n",
      " [0.33137783 0.33149695 0.33712518]\n",
      " [0.3314144  0.33153138 0.33705425]\n",
      " [0.3316087  0.3317139  0.3366774 ]\n",
      " [0.33130184 0.33142564 0.33727252]\n",
      " [0.33226615 0.33257416 0.33515966]\n",
      " [0.33208826 0.33235922 0.33555248]\n",
      " [0.33124658 0.33137366 0.33737975]\n",
      " [0.33208758 0.33255216 0.3353603 ]\n",
      " [0.3312451  0.33174694 0.33700797]\n",
      " [0.33191827 0.33240187 0.33567983]\n",
      " [0.3322925  0.33264574 0.3350618 ]\n",
      " [0.33178163 0.33200502 0.33621335]\n",
      " [0.33118954 0.33171433 0.33709612]\n",
      " [0.33259884 0.33302283 0.33437827]\n",
      " [0.3304825  0.33100843 0.33850905]\n",
      " [0.33145377 0.33198744 0.3365588 ]\n",
      " [0.33027318 0.33079988 0.33892694]\n",
      " [0.33067098 0.3312247  0.33810434]\n",
      " [0.3300957  0.33062178 0.33928254]\n",
      " [0.3305454  0.33110926 0.33834532]\n",
      " [0.32982445 0.3300874  0.34008813]\n",
      " [0.32997224 0.33051202 0.33951578]\n",
      " [0.32966125 0.33014154 0.34019718]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.333312   0.33331785 0.33337015]\n",
      " [0.3332824  0.33329433 0.33342326]\n",
      " [0.33322895 0.33323562 0.33353543]\n",
      " [0.33320716 0.3332304  0.3335625 ]\n",
      " [0.33323792 0.33323616 0.33352587]\n",
      " [0.33310738 0.33313218 0.3337604 ]\n",
      " [0.33307022 0.3331011  0.33382872]\n",
      " [0.33325028 0.33324873 0.333501  ]\n",
      " [0.3332064  0.333204   0.33358958]\n",
      " [0.33328602 0.33328512 0.33342886]\n",
      " [0.33315048 0.33314705 0.3337025 ]\n",
      " [0.3329146  0.3329416  0.33414382]\n",
      " [0.33330572 0.33330742 0.33338687]\n",
      " [0.33299652 0.33299026 0.33401325]\n",
      " [0.33324873 0.33324718 0.33350402]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33308855 0.33308402 0.3338275 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33323956 0.33324534 0.33351517]\n",
      " [0.3333211  0.33332184 0.33335707]\n",
      " [0.3329886  0.33298218 0.33402923]\n",
      " [0.33322963 0.333236   0.33353436]\n",
      " [0.3329142  0.3329399  0.33414587]\n",
      " [0.3333258  0.33332565 0.33334854]\n",
      " [0.3331667  0.3331769  0.3336564 ]\n",
      " [0.3331867  0.33319572 0.3336176 ]\n",
      " [0.33265233 0.33269405 0.33465365]\n",
      " [0.33274367 0.33277982 0.33447653]\n",
      " [0.33273575 0.33277237 0.33449188]\n",
      " [0.33299497 0.33301577 0.3339892 ]\n",
      " [0.33259597 0.33264112 0.3347629 ]\n",
      " [0.33280396 0.3328364  0.33435965]\n",
      " [0.33256435 0.33261153 0.33482412]\n",
      " [0.33264107 0.3326835  0.33467543]\n",
      " [0.33245182 0.33250576 0.33504242]\n",
      " [0.3324333  0.3324884  0.33507833]\n",
      " [0.33240172 0.33245876 0.3351395 ]\n",
      " [0.33240736 0.33246398 0.33512866]\n",
      " [0.33240825 0.33246487 0.3351269 ]\n",
      " [0.33242428 0.33247995 0.33509573]\n",
      " [0.33247864 0.33253098 0.33499038]\n",
      " [0.33243686 0.33249173 0.3350714 ]\n",
      " [0.33259022 0.33270478 0.33470494]\n",
      " [0.3324976  0.33275098 0.33475143]\n",
      " [0.3328265  0.33301556 0.33415794]\n",
      " [0.332275   0.33233973 0.33538526]\n",
      " [0.33296087 0.33319667 0.33384246]\n",
      " [0.33222917 0.33229667 0.33547413]\n",
      " [0.33252785 0.3326719  0.3348002 ]\n",
      " [0.33276892 0.33297873 0.3342524 ]\n",
      " [0.33182636 0.33212614 0.3360475 ]\n",
      " [0.33261582 0.33280617 0.334578  ]\n",
      " [0.3326592  0.33286628 0.33447453]\n",
      " [0.33273673 0.33302188 0.3342414 ]\n",
      " [0.33294362 0.33321092 0.33384544]\n",
      " [0.33143902 0.33175167 0.3368093 ]\n",
      " [0.33292195 0.33319804 0.33388   ]\n",
      " [0.33288798 0.33317685 0.33393514]\n",
      " [0.3320177  0.33236402 0.3356183 ]\n",
      " [0.3318693  0.33222404 0.33590662]\n",
      " [0.3312577  0.33137324 0.33736908]\n",
      " [0.33099666 0.3312369  0.33776647]\n",
      " [0.33104226 0.33136436 0.33759338]\n",
      " [0.33169854 0.33207747 0.336224  ]\n",
      " [0.33099547 0.33116752 0.33783698]\n",
      " [0.33116212 0.3315367  0.33730114]\n",
      " [0.33137318 0.33176714 0.3368597 ]\n",
      " [0.33151618 0.33151203 0.33697173]\n",
      " [0.33098668 0.3311166  0.3378968 ]\n",
      " [0.33159426 0.33156228 0.33684346]\n",
      " [0.33120334 0.33125412 0.3375425 ]\n",
      " [0.33184305 0.33181566 0.33634132]\n",
      " [0.3307623  0.3309318  0.3383059 ]\n",
      " [0.33064616 0.33085358 0.33850023]\n",
      " [0.3322377  0.33221748 0.3355448 ]\n",
      " [0.33095866 0.33103788 0.33800346]\n",
      " [0.33121476 0.33122364 0.3375616 ]\n",
      " [0.3316179  0.33158636 0.33679578]\n",
      " [0.33235294 0.33233485 0.33531222]\n",
      " [0.33213395 0.33211187 0.33575416]\n",
      " [0.33123726 0.3312172  0.3375455 ]\n",
      " [0.33224604 0.33222595 0.335528  ]\n",
      " [0.33073398 0.33082187 0.3384441 ]\n",
      " [0.332643   0.3326853  0.33467168]\n",
      " [0.33176622 0.33173737 0.33649647]\n",
      " [0.3326657  0.33265337 0.33468089]\n",
      " [0.33258396 0.33262986 0.33478615]\n",
      " [0.33321813 0.33322522 0.33355665]\n",
      " [0.33115816 0.33129063 0.33755118]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33158514 0.33169174 0.33672312]\n",
      " [0.33164808 0.3317509  0.336601  ]\n",
      " [0.3325413  0.3325898  0.33486897]\n",
      " [0.3310388  0.33117846 0.33778277]\n",
      " [0.33124867 0.33137563 0.33737573]\n",
      " [0.33089584 0.3310441  0.33806002]\n",
      " [0.33090425 0.33105195 0.33804384]\n",
      " [0.3316899  0.33196747 0.3363426 ]\n",
      " [0.33130136 0.33142513 0.3372735 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33330804 0.33330962 0.33338237]\n",
      " [0.33329406 0.33329743 0.33340853]\n",
      " [0.33327025 0.33327416 0.3334556 ]\n",
      " [0.3332536  0.3332598  0.3334866 ]\n",
      " [0.33321396 0.33322132 0.33356476]\n",
      " [0.33320415 0.3332121  0.3335837 ]\n",
      " [0.3331639  0.33317426 0.33366188]\n",
      " [0.3331956  0.3332171  0.33358726]\n",
      " [0.3331678  0.33318803 0.33364418]\n",
      " [0.3332537  0.33330116 0.33344513]\n",
      " [0.33320296 0.33324793 0.33354917]\n",
      " [0.333236   0.33329657 0.33346742]\n",
      " [0.3331357  0.3331786  0.33368567]\n",
      " [0.3330911  0.3331283  0.33378062]\n",
      " [0.33316708 0.33323067 0.33360222]\n",
      " [0.33320314 0.33328396 0.33351293]\n",
      " [0.33301747 0.33311516 0.33386737]\n",
      " [0.3326552  0.33273092 0.33461392]\n",
      " [0.33298573 0.33309466 0.33391964]\n",
      " [0.33318588 0.3332846  0.33352953]\n",
      " [0.33291492 0.3330368  0.3340483 ]\n",
      " [0.3328657  0.33299443 0.33413985]\n",
      " [0.33253425 0.33266005 0.33480567]\n",
      " [0.3330983  0.3332228  0.3336789 ]\n",
      " [0.33246732 0.33260447 0.33492824]\n",
      " [0.33238563 0.33251795 0.33509642]\n",
      " [0.33232272 0.3324474  0.33522984]\n",
      " [0.332375   0.33253023 0.33509478]\n",
      " [0.33225265 0.33239004 0.33535734]\n",
      " [0.3324288  0.33260477 0.33496645]\n",
      " [0.3321681  0.3323063  0.33552563]\n",
      " [0.33212683 0.33225957 0.3356136 ]\n",
      " [0.33229247 0.33233026 0.3353773 ]\n",
      " [0.33205846 0.33221373 0.33572778]\n",
      " [0.33255747 0.33254313 0.33489943]\n",
      " [0.3330142  0.3330083  0.3339775 ]\n",
      " [0.3319848  0.3321732  0.33584195]\n",
      " [0.3326163  0.33260304 0.33478063]\n",
      " [0.33285713 0.3328483  0.33429456]\n",
      " [0.33224013 0.33224344 0.33551645]\n",
      " [0.33244163 0.33242518 0.3351332 ]\n",
      " [0.33269045 0.3326786  0.33463097]\n",
      " [0.3326989  0.33268717 0.3346139 ]\n",
      " [0.33281255 0.33280295 0.33438447]\n",
      " [0.33208665 0.3320942  0.3358191 ]\n",
      " [0.33280307 0.33283556 0.33436134]\n",
      " [0.332724   0.3327614  0.3345146 ]\n",
      " [0.33299825 0.33299205 0.33400968]\n",
      " [0.33264235 0.3326296  0.334728  ]\n",
      " [0.3326594  0.3327007  0.33463994]\n",
      " [0.3331876  0.33319652 0.33361587]\n",
      " [0.33261853 0.33266228 0.33471915]\n",
      " [0.332757   0.33279234 0.33445066]\n",
      " [0.33214498 0.33221763 0.33563742]\n",
      " [0.332058   0.33213595 0.33580604]\n",
      " [0.33259934 0.3326443  0.33475634]\n",
      " [0.3328853  0.33291274 0.334202  ]\n",
      " [0.3328757  0.33290377 0.33422053]\n",
      " [0.331838   0.33192933 0.33623263]\n",
      " [0.3323348  0.33239588 0.33526936]\n",
      " [0.33191535 0.33200192 0.33608276]\n",
      " [0.33185872 0.3319488  0.33619252]\n",
      " [0.3317341  0.33183172 0.33643422]\n",
      " [0.33223137 0.33229876 0.33546984]\n",
      " [0.33177695 0.33187193 0.3363512 ]\n",
      " [0.33195767 0.33204168 0.33600068]\n",
      " [0.33183306 0.33192465 0.3362423 ]\n",
      " [0.3317378  0.33183518 0.33642706]\n",
      " [0.33196384 0.33207366 0.33596247]\n",
      " [0.33216667 0.33236736 0.33546597]\n",
      " [0.33178365 0.33187824 0.33633807]\n",
      " [0.3318535  0.332272   0.3358745 ]\n",
      " [0.33248958 0.33279195 0.33471844]\n",
      " [0.3319754  0.33214322 0.3358814 ]\n",
      " [0.3322133  0.3324662  0.33532047]\n",
      " [0.33262613 0.33301693 0.33435696]\n",
      " [0.33267447 0.33303285 0.3342927 ]\n",
      " [0.3316367  0.3317402  0.3366231 ]\n",
      " [0.33042997 0.33082396 0.3387461 ]\n",
      " [0.33229548 0.33272853 0.33497593]\n",
      " [0.3307313  0.33119565 0.33807305]\n",
      " [0.33253092 0.33295566 0.33451337]\n",
      " [0.3302108  0.3305839  0.33920535]\n",
      " [0.33074698 0.3312352  0.33801785]\n",
      " [0.3303225  0.33077574 0.33890173]\n",
      " [0.3318071  0.3322964  0.33589646]\n",
      " [0.33008122 0.3303814  0.33953738]\n",
      " [0.33000758 0.33037412 0.33961827]\n",
      " [0.33022955 0.33042318 0.3393473 ]\n",
      " [0.33029974 0.33080223 0.33889806]\n",
      " [0.33004197 0.33051187 0.33944616]\n",
      " [0.33188453 0.3318579  0.33625758]\n",
      " [0.3300717  0.33057317 0.3393551 ]\n",
      " [0.32983807 0.3301469  0.34001505]\n",
      " [0.3311187  0.33107808 0.33780318]\n",
      " [0.33014986 0.33030242 0.33954775]\n",
      " [0.3308996  0.33085918 0.33824125]\n",
      " [0.3300038  0.33018512 0.3398111 ]\n",
      " [0.33107477 0.3310333  0.33789197]]\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Initialize the nnfs library\n",
    "nnfs.init()\n",
    "\n",
    "# Define the Layer_Dense class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the output of the layer\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Define the Activation_Relu class\n",
    "class Activation_Relu:\n",
    "    def forward(self, inputs):\n",
    "        # Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Define the Activation_Softmax class\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Compute softmax activation\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Generate dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create instances of layers and activation functions\n",
    "dense1 = Layer_Dense(2, 3)       # Dense layer with 2 inputs and 3 neurons\n",
    "activation1 = Activation_Relu()   # ReLU activation function\n",
    "dense2 = Layer_Dense(3, 3)       # Dense layer with 3 inputs and 3 neurons\n",
    "activation2 = Activation_Softmax()  # Softmax activation function\n",
    "\n",
    "# Perform forward pass through the network\n",
    "dense1.forward(X)              # Forward pass through the first dense layer\n",
    "activation1.forward(dense1.output)  # Forward pass through the ReLU activation\n",
    "dense2.forward(activation1.output)  # Forward pass through the second dense layer\n",
    "activation2.forward(dense2.output)  # Forward pass through the Softmax activation\n",
    "\n",
    "# Print the output of the Softmax activation\n",
    "print(activation2.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Loss and Categorical Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss:\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Softmax output probabilities\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# Target output (one-hot encoded)\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "loss = -(math.log(softmax_output[0]) * target_output[0] +\n",
    "         math.log(softmax_output[1]) * target_output[1] +\n",
    "         math.log(softmax_output[2]) * target_output[2])\n",
    "\n",
    "print(\"Cross-entropy loss:\")\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss:\n",
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ishaa\\AppData\\Local\\Temp\\ipykernel_7080\\2826406926.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.mean(-np.log(softmax_output[np.arange(len(class_targets)), class_targets]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Softmax output probabilities for multiple samples\n",
    "softmax_output = np.array([\n",
    "    [0.0, 0.1, 0.2],   # Probabilities for sample 1\n",
    "    [0.1, 0.5, 0.4],   # Probabilities for sample 2\n",
    "    [0.02, 0.9, 0.08]  # Probabilities for sample 3\n",
    "])\n",
    "\n",
    "# True class labels for each sample (0-based index)\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "# Select the probabilities corresponding to the true class labels\n",
    "# Apply the negative log transformation and calculate the mean loss\n",
    "loss = np.mean(-np.log(softmax_output[np.arange(len(class_targets)), class_targets]))\n",
    "\n",
    "print(\"Cross-entropy loss:\")\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output:\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]\n",
      " [0.33328417 0.33328718 0.33342862]\n",
      " [0.33318216 0.33319145 0.33362636]\n",
      " [0.33318278 0.33319202 0.33362517]\n",
      " [0.33314922 0.33316055 0.3336902 ]\n",
      " [0.3331059  0.33311984 0.3337743 ]\n",
      " [0.3330813  0.3330968  0.33382186]\n",
      " [0.33311027 0.33312503 0.33376473]\n",
      " [0.3330537  0.33307084 0.33387548]\n",
      " [0.33300948 0.33302936 0.3339612 ]\n",
      " [0.33301342 0.33303303 0.33395353]\n",
      " [0.33299845 0.333019   0.3339825 ]\n",
      " [0.33312678 0.33318865 0.3336846 ]\n",
      " [0.3329409  0.33296496 0.33409408]\n",
      " [0.33299428 0.33303145 0.33397427]\n",
      " [0.3328737  0.33290187 0.33422446]\n",
      " [0.3328757  0.33290377 0.33422053]\n",
      " [0.33309552 0.33318356 0.33372095]\n",
      " [0.33317605 0.33328283 0.3335411 ]\n",
      " [0.332894   0.33293876 0.33416727]\n",
      " [0.3325638  0.33270204 0.3347342 ]\n",
      " [0.33281362 0.33284548 0.3343409 ]\n",
      " [0.33314455 0.33327034 0.33358508]\n",
      " [0.3331285  0.333258   0.33361349]\n",
      " [0.33241627 0.33257595 0.33500782]\n",
      " [0.33234823 0.33250993 0.3351418 ]\n",
      " [0.3327268  0.33290118 0.334372  ]\n",
      " [0.33251256 0.3326966  0.33479083]\n",
      " [0.33303848 0.33320293 0.33375862]\n",
      " [0.33289513 0.33302    0.33408487]\n",
      " [0.3324894  0.33269098 0.3348196 ]\n",
      " [0.33217454 0.33237296 0.33545244]\n",
      " [0.33198893 0.3321154  0.3358957 ]\n",
      " [0.33197358 0.33208773 0.33593866]\n",
      " [0.33205435 0.3322675  0.33567816]\n",
      " [0.33193922 0.33214495 0.33591586]\n",
      " [0.33208537 0.33231774 0.3355969 ]\n",
      " [0.33213225 0.33237374 0.33549398]\n",
      " [0.33214602 0.33239442 0.33545953]\n",
      " [0.33266547 0.3326531  0.33468142]\n",
      " [0.33167848 0.33184558 0.33647597]\n",
      " [0.3316745  0.33181593 0.33650956]\n",
      " [0.33160535 0.3318128  0.33658186]\n",
      " [0.33236307 0.33234516 0.33529174]\n",
      " [0.33152688 0.3317393  0.33673382]\n",
      " [0.33195725 0.3319697  0.33607304]\n",
      " [0.33166933 0.33175212 0.33657855]\n",
      " [0.33258075 0.3325668  0.33485246]\n",
      " [0.3320807  0.33205754 0.3358618 ]\n",
      " [0.33174273 0.3317812  0.33647612]\n",
      " [0.3326942  0.33268243 0.33462337]\n",
      " [0.33327198 0.33327082 0.33345714]\n",
      " [0.33312216 0.33311826 0.33375955]\n",
      " [0.33332214 0.33332282 0.33335507]\n",
      " [0.3328328  0.33282354 0.33434367]\n",
      " [0.33328426 0.33328333 0.3334324 ]\n",
      " [0.33317587 0.33317295 0.33365116]\n",
      " [0.33320865 0.33321625 0.33357516]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33270696 0.33269536 0.33459768]\n",
      " [0.3323478  0.33240813 0.3352441 ]\n",
      " [0.33259606 0.33264124 0.33476272]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3318191  0.33191153 0.33626938]\n",
      " [0.3324854  0.33253726 0.33497733]\n",
      " [0.33177653 0.33187154 0.33635193]\n",
      " [0.33155695 0.3316653  0.33677778]\n",
      " [0.33158025 0.33168715 0.3367326 ]\n",
      " [0.33162922 0.3317332  0.33663756]\n",
      " [0.33193487 0.3320203  0.3360448 ]\n",
      " [0.33149302 0.33160523 0.33690175]\n",
      " [0.33148253 0.33159536 0.33692214]\n",
      " [0.3314542  0.33156872 0.3369771 ]\n",
      " [0.33137783 0.33149695 0.33712518]\n",
      " [0.3314144  0.33153138 0.33705425]\n",
      " [0.3316087  0.3317139  0.3366774 ]\n",
      " [0.33130184 0.33142564 0.33727252]\n",
      " [0.33226615 0.33257416 0.33515966]\n",
      " [0.33208826 0.33235922 0.33555248]\n",
      " [0.33124658 0.33137366 0.33737975]\n",
      " [0.33208758 0.33255216 0.3353603 ]\n",
      " [0.3312451  0.33174694 0.33700797]\n",
      " [0.33191827 0.33240187 0.33567983]\n",
      " [0.3322925  0.33264574 0.3350618 ]\n",
      " [0.33178163 0.33200502 0.33621335]\n",
      " [0.33118954 0.33171433 0.33709612]\n",
      " [0.33259884 0.33302283 0.33437827]\n",
      " [0.3304825  0.33100843 0.33850905]\n",
      " [0.33145377 0.33198744 0.3365588 ]\n",
      " [0.33027318 0.33079988 0.33892694]\n",
      " [0.33067098 0.3312247  0.33810434]\n",
      " [0.3300957  0.33062178 0.33928254]\n",
      " [0.3305454  0.33110926 0.33834532]\n",
      " [0.32982445 0.3300874  0.34008813]\n",
      " [0.32997224 0.33051202 0.33951578]\n",
      " [0.32966125 0.33014154 0.34019718]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.333312   0.33331785 0.33337015]\n",
      " [0.3332824  0.33329433 0.33342326]\n",
      " [0.33322895 0.33323562 0.33353543]\n",
      " [0.33320716 0.3332304  0.3335625 ]\n",
      " [0.33323792 0.33323616 0.33352587]\n",
      " [0.33310738 0.33313218 0.3337604 ]\n",
      " [0.33307022 0.3331011  0.33382872]\n",
      " [0.33325028 0.33324873 0.333501  ]\n",
      " [0.3332064  0.333204   0.33358958]\n",
      " [0.33328602 0.33328512 0.33342886]\n",
      " [0.33315048 0.33314705 0.3337025 ]\n",
      " [0.3329146  0.3329416  0.33414382]\n",
      " [0.33330572 0.33330742 0.33338687]\n",
      " [0.33299652 0.33299026 0.33401325]\n",
      " [0.33324873 0.33324718 0.33350402]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33308855 0.33308402 0.3338275 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33323956 0.33324534 0.33351517]\n",
      " [0.3333211  0.33332184 0.33335707]\n",
      " [0.3329886  0.33298218 0.33402923]\n",
      " [0.33322963 0.333236   0.33353436]\n",
      " [0.3329142  0.3329399  0.33414587]\n",
      " [0.3333258  0.33332565 0.33334854]\n",
      " [0.3331667  0.3331769  0.3336564 ]\n",
      " [0.3331867  0.33319572 0.3336176 ]\n",
      " [0.33265233 0.33269405 0.33465365]\n",
      " [0.33274367 0.33277982 0.33447653]\n",
      " [0.33273575 0.33277237 0.33449188]\n",
      " [0.33299497 0.33301577 0.3339892 ]\n",
      " [0.33259597 0.33264112 0.3347629 ]\n",
      " [0.33280396 0.3328364  0.33435965]\n",
      " [0.33256435 0.33261153 0.33482412]\n",
      " [0.33264107 0.3326835  0.33467543]\n",
      " [0.33245182 0.33250576 0.33504242]\n",
      " [0.3324333  0.3324884  0.33507833]\n",
      " [0.33240172 0.33245876 0.3351395 ]\n",
      " [0.33240736 0.33246398 0.33512866]\n",
      " [0.33240825 0.33246487 0.3351269 ]\n",
      " [0.33242428 0.33247995 0.33509573]\n",
      " [0.33247864 0.33253098 0.33499038]\n",
      " [0.33243686 0.33249173 0.3350714 ]\n",
      " [0.33259022 0.33270478 0.33470494]\n",
      " [0.3324976  0.33275098 0.33475143]\n",
      " [0.3328265  0.33301556 0.33415794]\n",
      " [0.332275   0.33233973 0.33538526]\n",
      " [0.33296087 0.33319667 0.33384246]\n",
      " [0.33222917 0.33229667 0.33547413]\n",
      " [0.33252785 0.3326719  0.3348002 ]\n",
      " [0.33276892 0.33297873 0.3342524 ]\n",
      " [0.33182636 0.33212614 0.3360475 ]\n",
      " [0.33261582 0.33280617 0.334578  ]\n",
      " [0.3326592  0.33286628 0.33447453]\n",
      " [0.33273673 0.33302188 0.3342414 ]\n",
      " [0.33294362 0.33321092 0.33384544]\n",
      " [0.33143902 0.33175167 0.3368093 ]\n",
      " [0.33292195 0.33319804 0.33388   ]\n",
      " [0.33288798 0.33317685 0.33393514]\n",
      " [0.3320177  0.33236402 0.3356183 ]\n",
      " [0.3318693  0.33222404 0.33590662]\n",
      " [0.3312577  0.33137324 0.33736908]\n",
      " [0.33099666 0.3312369  0.33776647]\n",
      " [0.33104226 0.33136436 0.33759338]\n",
      " [0.33169854 0.33207747 0.336224  ]\n",
      " [0.33099547 0.33116752 0.33783698]\n",
      " [0.33116212 0.3315367  0.33730114]\n",
      " [0.33137318 0.33176714 0.3368597 ]\n",
      " [0.33151618 0.33151203 0.33697173]\n",
      " [0.33098668 0.3311166  0.3378968 ]\n",
      " [0.33159426 0.33156228 0.33684346]\n",
      " [0.33120334 0.33125412 0.3375425 ]\n",
      " [0.33184305 0.33181566 0.33634132]\n",
      " [0.3307623  0.3309318  0.3383059 ]\n",
      " [0.33064616 0.33085358 0.33850023]\n",
      " [0.3322377  0.33221748 0.3355448 ]\n",
      " [0.33095866 0.33103788 0.33800346]\n",
      " [0.33121476 0.33122364 0.3375616 ]\n",
      " [0.3316179  0.33158636 0.33679578]\n",
      " [0.33235294 0.33233485 0.33531222]\n",
      " [0.33213395 0.33211187 0.33575416]\n",
      " [0.33123726 0.3312172  0.3375455 ]\n",
      " [0.33224604 0.33222595 0.335528  ]\n",
      " [0.33073398 0.33082187 0.3384441 ]\n",
      " [0.332643   0.3326853  0.33467168]\n",
      " [0.33176622 0.33173737 0.33649647]\n",
      " [0.3326657  0.33265337 0.33468089]\n",
      " [0.33258396 0.33262986 0.33478615]\n",
      " [0.33321813 0.33322522 0.33355665]\n",
      " [0.33115816 0.33129063 0.33755118]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33158514 0.33169174 0.33672312]\n",
      " [0.33164808 0.3317509  0.336601  ]\n",
      " [0.3325413  0.3325898  0.33486897]\n",
      " [0.3310388  0.33117846 0.33778277]\n",
      " [0.33124867 0.33137563 0.33737573]\n",
      " [0.33089584 0.3310441  0.33806002]\n",
      " [0.33090425 0.33105195 0.33804384]\n",
      " [0.3316899  0.33196747 0.3363426 ]\n",
      " [0.33130136 0.33142513 0.3372735 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33330804 0.33330962 0.33338237]\n",
      " [0.33329406 0.33329743 0.33340853]\n",
      " [0.33327025 0.33327416 0.3334556 ]\n",
      " [0.3332536  0.3332598  0.3334866 ]\n",
      " [0.33321396 0.33322132 0.33356476]\n",
      " [0.33320415 0.3332121  0.3335837 ]\n",
      " [0.3331639  0.33317426 0.33366188]\n",
      " [0.3331956  0.3332171  0.33358726]\n",
      " [0.3331678  0.33318803 0.33364418]\n",
      " [0.3332537  0.33330116 0.33344513]\n",
      " [0.33320296 0.33324793 0.33354917]\n",
      " [0.333236   0.33329657 0.33346742]\n",
      " [0.3331357  0.3331786  0.33368567]\n",
      " [0.3330911  0.3331283  0.33378062]\n",
      " [0.33316708 0.33323067 0.33360222]\n",
      " [0.33320314 0.33328396 0.33351293]\n",
      " [0.33301747 0.33311516 0.33386737]\n",
      " [0.3326552  0.33273092 0.33461392]\n",
      " [0.33298573 0.33309466 0.33391964]\n",
      " [0.33318588 0.3332846  0.33352953]\n",
      " [0.33291492 0.3330368  0.3340483 ]\n",
      " [0.3328657  0.33299443 0.33413985]\n",
      " [0.33253425 0.33266005 0.33480567]\n",
      " [0.3330983  0.3332228  0.3336789 ]\n",
      " [0.33246732 0.33260447 0.33492824]\n",
      " [0.33238563 0.33251795 0.33509642]\n",
      " [0.33232272 0.3324474  0.33522984]\n",
      " [0.332375   0.33253023 0.33509478]\n",
      " [0.33225265 0.33239004 0.33535734]\n",
      " [0.3324288  0.33260477 0.33496645]\n",
      " [0.3321681  0.3323063  0.33552563]\n",
      " [0.33212683 0.33225957 0.3356136 ]\n",
      " [0.33229247 0.33233026 0.3353773 ]\n",
      " [0.33205846 0.33221373 0.33572778]\n",
      " [0.33255747 0.33254313 0.33489943]\n",
      " [0.3330142  0.3330083  0.3339775 ]\n",
      " [0.3319848  0.3321732  0.33584195]\n",
      " [0.3326163  0.33260304 0.33478063]\n",
      " [0.33285713 0.3328483  0.33429456]\n",
      " [0.33224013 0.33224344 0.33551645]\n",
      " [0.33244163 0.33242518 0.3351332 ]\n",
      " [0.33269045 0.3326786  0.33463097]\n",
      " [0.3326989  0.33268717 0.3346139 ]\n",
      " [0.33281255 0.33280295 0.33438447]\n",
      " [0.33208665 0.3320942  0.3358191 ]\n",
      " [0.33280307 0.33283556 0.33436134]\n",
      " [0.332724   0.3327614  0.3345146 ]\n",
      " [0.33299825 0.33299205 0.33400968]\n",
      " [0.33264235 0.3326296  0.334728  ]\n",
      " [0.3326594  0.3327007  0.33463994]\n",
      " [0.3331876  0.33319652 0.33361587]\n",
      " [0.33261853 0.33266228 0.33471915]\n",
      " [0.332757   0.33279234 0.33445066]\n",
      " [0.33214498 0.33221763 0.33563742]\n",
      " [0.332058   0.33213595 0.33580604]\n",
      " [0.33259934 0.3326443  0.33475634]\n",
      " [0.3328853  0.33291274 0.334202  ]\n",
      " [0.3328757  0.33290377 0.33422053]\n",
      " [0.331838   0.33192933 0.33623263]\n",
      " [0.3323348  0.33239588 0.33526936]\n",
      " [0.33191535 0.33200192 0.33608276]\n",
      " [0.33185872 0.3319488  0.33619252]\n",
      " [0.3317341  0.33183172 0.33643422]\n",
      " [0.33223137 0.33229876 0.33546984]\n",
      " [0.33177695 0.33187193 0.3363512 ]\n",
      " [0.33195767 0.33204168 0.33600068]\n",
      " [0.33183306 0.33192465 0.3362423 ]\n",
      " [0.3317378  0.33183518 0.33642706]\n",
      " [0.33196384 0.33207366 0.33596247]\n",
      " [0.33216667 0.33236736 0.33546597]\n",
      " [0.33178365 0.33187824 0.33633807]\n",
      " [0.3318535  0.332272   0.3358745 ]\n",
      " [0.33248958 0.33279195 0.33471844]\n",
      " [0.3319754  0.33214322 0.3358814 ]\n",
      " [0.3322133  0.3324662  0.33532047]\n",
      " [0.33262613 0.33301693 0.33435696]\n",
      " [0.33267447 0.33303285 0.3342927 ]\n",
      " [0.3316367  0.3317402  0.3366231 ]\n",
      " [0.33042997 0.33082396 0.3387461 ]\n",
      " [0.33229548 0.33272853 0.33497593]\n",
      " [0.3307313  0.33119565 0.33807305]\n",
      " [0.33253092 0.33295566 0.33451337]\n",
      " [0.3302108  0.3305839  0.33920535]\n",
      " [0.33074698 0.3312352  0.33801785]\n",
      " [0.3303225  0.33077574 0.33890173]\n",
      " [0.3318071  0.3322964  0.33589646]\n",
      " [0.33008122 0.3303814  0.33953738]\n",
      " [0.33000758 0.33037412 0.33961827]\n",
      " [0.33022955 0.33042318 0.3393473 ]\n",
      " [0.33029974 0.33080223 0.33889806]\n",
      " [0.33004197 0.33051187 0.33944616]\n",
      " [0.33188453 0.3318579  0.33625758]\n",
      " [0.3300717  0.33057317 0.3393551 ]\n",
      " [0.32983807 0.3301469  0.34001505]\n",
      " [0.3311187  0.33107808 0.33780318]\n",
      " [0.33014986 0.33030242 0.33954775]\n",
      " [0.3308996  0.33085918 0.33824125]\n",
      " [0.3300038  0.33018512 0.3398111 ]\n",
      " [0.33107477 0.3310333  0.33789197]]\n",
      "Categorical Cross-Entropy Loss:\n",
      "1.098445\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Initialize the nnfs library\n",
    "nnfs.init()\n",
    "\n",
    "# Define the Dense Layer class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases as zeros\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Compute the output of the layer\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Define the ReLU Activation class\n",
    "class Activation_Relu:\n",
    "    def forward(self, inputs):\n",
    "        # Apply ReLU activation function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "# Define the Softmax Activation class\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Compute softmax activation\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "# Define the Loss base class\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the loss value\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "# Define the Categorical Cross-Entropy Loss class\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            # If y_true is a vector of class indices\n",
    "            correct_confidences = y_pred_clipped[np.arange(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # If y_true is a one-hot encoded matrix\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        # Compute negative log likelihoods\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "# Generate dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create instances of layers and activation functions\n",
    "dense1 = Layer_Dense(2, 3)           # Dense layer with 2 inputs and 3 neurons\n",
    "activation1 = Activation_Relu()      # ReLU activation function\n",
    "dense2 = Layer_Dense(3, 3)           # Dense layer with 3 inputs and 3 neurons\n",
    "activation2 = Activation_Softmax()   # Softmax activation function\n",
    "\n",
    "# Perform forward pass through the network\n",
    "dense1.forward(X)              # Forward pass through the first dense layer\n",
    "activation1.forward(dense1.output)  # Forward pass through the ReLU activation\n",
    "dense2.forward(activation1.output)  # Forward pass through the second dense layer\n",
    "activation2.forward(dense2.output)  # Forward pass through the Softmax activation\n",
    "\n",
    "# Print the output of the Softmax activation\n",
    "print(\"Softmax output:\")\n",
    "print(activation2.output)\n",
    "\n",
    "# Calculate the loss using categorical cross-entropy\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(\"Categorical Cross-Entropy Loss:\")\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#probabilities of 3 samples\n",
    "softmax_outputs=np.array([[0.7,0.2,0.1],\n",
    "                          [0.5,0.1,0.4],\n",
    "                          [0.02,0.9,0.08]])\n",
    "#Target (ground-truth) labels for 3 samples\n",
    "class_targets=np.array([[0,1,1]])\n",
    "\n",
    "#calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs,axis=1)\n",
    "\n",
    "#if targets are one-hot encoded - convert them\n",
    "if len(class_targets.shape)==2:\n",
    "    class_targets=np.argmax(class_targets,axis=1)\n",
    "\n",
    "#True evaluates to 1; Flase to 0\n",
    "accuracy=np.mean(predictions==class_targets)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate accuracy from output of activation2 and targets\n",
    "#Calculate values along first axis\n",
    "predictions =np.argmax(activation2.output,axis=1)\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y,axis=1)\n",
    "accuracy=np.mean(predictions==y)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
